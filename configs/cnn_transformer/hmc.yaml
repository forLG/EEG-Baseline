# Global settings
seed: 42
device: cuda

# Training hyperparameters
training:
  epochs: 20

  optimizer:
    name: AdamW
    params: 
      lr: 0.00005 # will be overwriten by lr_scheduler
      weight_decay: 0.05

  lr_scheduler:
    name: CosineAnnealingLR
    params:
      T_max: 20
      eta_min: 0.000005

  criterion: CrossEntropyLoss # fixed

  metrics: [acc, balanced_acc, f1]

  checkpoint:
    dir: ./checkpoints/CNN_Transformer2
    save_strategy: best # best, last, epoch
    monitor: balanced_acc # One of metric
    save_freq: 1 # per epoch, used if save_strategy=epoch
  
  logging:
    dir: ./logs
    project_name: CNN_Transformer-HMC
    log_freq: 1 # per epoch
    report_to: None # None, tensorboard, wandb
  # ...

# Evaluation settings
evaluation:
  output_dir: ./outputs

# Model and dataset configuration
model:
  name: cnn_transformer
  class_name: CNNTransformer
  params:
    num_classes: 4 # tusz
    num_channels: 4 # tusz
    L_seconds: 10
    fs: 200 # A sample is 10 * 200 = 2000 points
    # pretrain_model_path: None # pass by command line when evaluation

dataset:
  name: hmc
  path: "./data/HMC"
  batch_size: 32
  num_workers: 4