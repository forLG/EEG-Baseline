# Global settings
seed: 42
device: cuda

# Training hyperparameters
training:
  epochs: 20

  optimizer:
    name: AdamW
    params: 
      lr: 0.00005 # will be overwriten by lr_scheduler
      weight_decay: 0.05

  lr_scheduler:
    name: CosineAnnealingLR
    params:
      T_max: 20
      eta_min: 0.000005

  criterion: CrossEntropyLoss # fixed

  metrics: [acc, balanced_acc, f1]

  checkpoint:
    dir: ./checkpoints/EEGNet
    save_strategy: best # best, last, epoch
    monitor: balanced_acc # One of metric
    save_freq: 1 # per epoch, used if save_strategy=epoch
  
  logging:
    dir: ./logs
    project_name: EEGNet-HMC
    log_freq: 1 # per epoch
    report_to: None # None, tensorboard, wandb
  # ...

# Evaluation settings
evaluation:
  output_dir: ./outputs

# Model and dataset configuration
model:
  name: eegnet
  class_name: EEGNet
  params:
    num_classes: 4 # hmc
    num_channels: 4 # hmc
    window_length: 2000
    num_temporal_filts: 8
    num_spatial_filts: 2
    p_dropout: 0.5
    avgpool_factor: 4
    # pretrain_model_path: None # pass by command line when evaluation

dataset:
  name: hmc
  path: ./data/HMC
  batch_size: 32
  num_workers: 4